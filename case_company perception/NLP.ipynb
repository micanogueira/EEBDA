{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Natural Language Processing describes the translation of human language – in written or spoken form – into a format that machines can understand."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a2c531dc9454378"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "462eddcf5bd892bd"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 13:15:55.625435: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eebda': 1, 'is': 2, 'house': 3, 'fun': 4, 'a': 5, 'great': 6, 'course': 7, 'houses': 8}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    \"EEBDA is fun!\",\n",
    "    \"EEBDA is a great course.\",\n",
    "    \"house\",\n",
    "    \"House\",\n",
    "    \"Houses\"]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 11) # the maximum number of words to keep, based on word frequency\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index # the words and the assigned integers are stored\n",
    "print(word_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T12:16:10.460497Z",
     "start_time": "2023-12-27T12:15:41.459519Z"
    }
   },
   "id": "6329bc310ed974dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The more frequent words like “EEBDA” or “is” are assigned lower integers. Upper case letters are converted to lower case letters. Consequently, the words “house” and “House” are mapped as a single token (`“house”: 3`), which makes sense since each expression provide the same information content for subsequent model evaluations. Finally, it should be noted that “houses” is assigned to its own token. Thus, plural forms of words are not automatically assigned to the singular form."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95d68c23e89115df"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mouse': 1, 'mice': 2}\n"
     ]
    }
   ],
   "source": [
    "sensitive_cases = [ 'mouse', 'Mouse', 'Mice']\n",
    "\n",
    "tokenizer_sensitive_cases = Tokenizer(num_words = 10) \n",
    "tokenizer_sensitive_cases.fit_on_texts(sensitive_cases)\n",
    "\n",
    "print(tokenizer_sensitive_cases.word_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T12:48:27.637204Z",
     "start_time": "2023-12-27T12:48:27.620302Z"
    }
   },
   "id": "ec99aa650a394b74"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `num_words` parameter is the maximum number of words to keep. In the example below, we want the most frequent 100 words. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acd45c2bf668a7c2"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
     ]
    }
   ],
   "source": [
    "sentences_2 = [\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat\"\n",
    "]\n",
    "\n",
    "# We create an instance of a tokenizer object\n",
    "tokenizer_2 = Tokenizer(num_words = 100)\n",
    "\n",
    "# Go through the text\n",
    "tokenizer_2.fit_on_texts(sentences_2)\n",
    "\n",
    "print(tokenizer_2.word_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T12:23:55.081673Z",
     "start_time": "2023-12-27T12:23:55.072360Z"
    }
   },
   "id": "f587f1c9de6155"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `Tokenizer` is smart enough to catch some exceptions. For example, if we update our `sentences_2` by adding a third sentence with `\"dog!\"`, the exclamation point will be spotted and a new token will not be created:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d08a0d38caf45e0"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "sentences_2 = [\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat\",\n",
    "    \"You love my dog!\"\n",
    "]\n",
    "\n",
    "tokenizer_2.fit_on_texts(sentences_2)\n",
    "print(tokenizer_2.word_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T12:24:01.799067Z",
     "start_time": "2023-12-27T12:24:01.782455Z"
    }
   },
   "id": "2aeaec1cce5799dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    ">  By default, all punctuation is removed, turning the texts into space-separated sequences of words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d85cdb1db850c463"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Tokenizer in module keras.src.preprocessing.text:\n",
      "\n",
      "class Tokenizer(builtins.object)\n",
      " |  Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None, analyzer=None, **kwargs)\n",
      " |  \n",
      " |  Text tokenization utility class.\n",
      " |  \n",
      " |  Deprecated: `tf.keras.preprocessing.text.Tokenizer` does not operate on\n",
      " |  tensors and is not recommended for new code. Prefer\n",
      " |  `tf.keras.layers.TextVectorization` which provides equivalent functionality\n",
      " |  through a layer which accepts `tf.Tensor` input. See the\n",
      " |  [text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text)\n",
      " |  for an overview of the layer and text handling in tensorflow.\n",
      " |  \n",
      " |  This class allows to vectorize a text corpus, by turning each\n",
      " |  text into either a sequence of integers (each integer being the index\n",
      " |  of a token in a dictionary) or into a vector where the coefficient\n",
      " |  for each token could be binary, based on word count, based on tf-idf...\n",
      " |  \n",
      " |  By default, all punctuation is removed, turning the texts into\n",
      " |  space-separated sequences of words\n",
      " |  (words may include the `'` character). These sequences are then\n",
      " |  split into lists of tokens. They will then be indexed or vectorized.\n",
      " |  \n",
      " |  `0` is a reserved index that won't be assigned to any word.\n",
      " |  \n",
      " |  Args:\n",
      " |      num_words: the maximum number of words to keep, based\n",
      " |          on word frequency. Only the most common `num_words-1` words will\n",
      " |          be kept.\n",
      " |      filters: a string where each element is a character that will be\n",
      " |          filtered from the texts. The default is all punctuation, plus\n",
      " |          tabs and line breaks, minus the `'` character.\n",
      " |      lower: boolean. Whether to convert the texts to lowercase.\n",
      " |      split: str. Separator for word splitting.\n",
      " |      char_level: if True, every character will be treated as a token.\n",
      " |      oov_token: if given, it will be added to word_index and used to\n",
      " |          replace out-of-vocabulary words during text_to_sequence calls\n",
      " |      analyzer: function. Custom analyzer to split the text.\n",
      " |          The default analyzer is text_to_word_sequence\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=None, analyzer=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit_on_sequences(self, sequences)\n",
      " |      Updates internal vocabulary based on a list of sequences.\n",
      " |      \n",
      " |      Required before using `sequences_to_matrix`\n",
      " |      (if `fit_on_texts` was never called).\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences: A list of sequence.\n",
      " |              A \"sequence\" is a list of integer word indices.\n",
      " |  \n",
      " |  fit_on_texts(self, texts)\n",
      " |      Updates internal vocabulary based on a list of texts.\n",
      " |      \n",
      " |      In the case where texts contains lists,\n",
      " |      we assume each entry of the lists to be a token.\n",
      " |      \n",
      " |      Required before using `texts_to_sequences` or `texts_to_matrix`.\n",
      " |      \n",
      " |      Args:\n",
      " |          texts: can be a list of strings,\n",
      " |              a generator of strings (for memory-efficiency),\n",
      " |              or a list of list of strings.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the tokenizer configuration as Python dictionary.\n",
      " |      \n",
      " |      The word count dictionaries used by the tokenizer get serialized\n",
      " |      into plain JSON, so that the configuration can be read by other\n",
      " |      projects.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A Python dictionary with the tokenizer configuration.\n",
      " |  \n",
      " |  sequences_to_matrix(self, sequences, mode='binary')\n",
      " |      Converts a list of sequences into a Numpy matrix.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences: list of sequences\n",
      " |              (a sequence is a list of integer word indices).\n",
      " |          mode: one of \"binary\", \"count\", \"tfidf\", \"freq\"\n",
      " |      \n",
      " |      Returns:\n",
      " |          A Numpy matrix.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case of invalid `mode` argument,\n",
      " |              or if the Tokenizer requires to be fit to sample data.\n",
      " |  \n",
      " |  sequences_to_texts(self, sequences)\n",
      " |      Transforms each sequence into a list of text.\n",
      " |      \n",
      " |      Only top `num_words-1` most frequent words will be taken into account.\n",
      " |      Only words known by the tokenizer will be taken into account.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences: A list of sequences (list of integers).\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of texts (strings)\n",
      " |  \n",
      " |  sequences_to_texts_generator(self, sequences)\n",
      " |      Transforms each sequence in `sequences` to a list of texts(strings).\n",
      " |      \n",
      " |      Each sequence has to a list of integers.\n",
      " |      In other words, sequences should be a list of sequences\n",
      " |      \n",
      " |      Only top `num_words-1` most frequent words will be taken into account.\n",
      " |      Only words known by the tokenizer will be taken into account.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences: A list of sequences.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Yields individual texts.\n",
      " |  \n",
      " |  texts_to_matrix(self, texts, mode='binary')\n",
      " |      Convert a list of texts to a Numpy matrix.\n",
      " |      \n",
      " |      Args:\n",
      " |          texts: list of strings.\n",
      " |          mode: one of \"binary\", \"count\", \"tfidf\", \"freq\".\n",
      " |      \n",
      " |      Returns:\n",
      " |          A Numpy matrix.\n",
      " |  \n",
      " |  texts_to_sequences(self, texts)\n",
      " |      Transforms each text in texts to a sequence of integers.\n",
      " |      \n",
      " |      Only top `num_words-1` most frequent words will be taken into account.\n",
      " |      Only words known by the tokenizer will be taken into account.\n",
      " |      \n",
      " |      Args:\n",
      " |          texts: A list of texts (strings).\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of sequences.\n",
      " |  \n",
      " |  texts_to_sequences_generator(self, texts)\n",
      " |      Transforms each text in `texts` to a sequence of integers.\n",
      " |      \n",
      " |      Each item in texts can also be a list,\n",
      " |      in which case we assume each item of that list to be a token.\n",
      " |      \n",
      " |      Only top `num_words-1` most frequent words will be taken into account.\n",
      " |      Only words known by the tokenizer will be taken into account.\n",
      " |      \n",
      " |      Args:\n",
      " |          texts: A list of texts (strings).\n",
      " |      \n",
      " |      Yields:\n",
      " |          Yields individual sequences.\n",
      " |  \n",
      " |  to_json(self, **kwargs)\n",
      " |      Returns a JSON string containing the tokenizer configuration.\n",
      " |      \n",
      " |      To load a tokenizer from a JSON string, use\n",
      " |      `keras.preprocessing.text.tokenizer_from_json(json_string)`.\n",
      " |      \n",
      " |      Args:\n",
      " |          **kwargs: Additional keyword arguments\n",
      " |              to be passed to `json.dumps()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A JSON string containing the tokenizer configuration.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n"
     ]
    }
   ],
   "source": [
    "help(Tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T12:25:19.050240Z",
     "start_time": "2023-12-27T12:25:19.039825Z"
    }
   },
   "id": "35f198332053ef2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also manipulate the tokenizer in such a way that the punctuation marks are not filtered:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b9494c80705dc4c"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog-': 4, 'cat?': 5, 'you': 6, 'dog!': 7}\n"
     ]
    }
   ],
   "source": [
    "sentences_2 = [\n",
    "    \"I love my dog-\",\n",
    "    \"I love my cat?\",\n",
    "    \"You love my dog!\"\n",
    "]\n",
    "\n",
    "tokenizer_2 = Tokenizer(num_words = 100, filters=\".\") # filtering any single character\n",
    "tokenizer_2.fit_on_texts(sentences_2)\n",
    "print(tokenizer_2.word_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-27T12:35:07.750054Z"
    }
   },
   "id": "68e65bc4b1176046"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stemming and Lemmatization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62202890b2c0bddd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The word “House” would be created from the word “Houses” after the application of stemming methods. Lemmatization allows correct transfer to the root word by referring to a dictionary."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68f46fbe24fbd7e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   Token   Base\n0  EEBDA  EEBDA\n1     is     be\n2     SO     so\n3   much   much\n4    fun    fun\n5      !      !",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Token</th>\n      <th>Base</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>EEBDA</td>\n      <td>EEBDA</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>is</td>\n      <td>be</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SO</td>\n      <td>so</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>much</td>\n      <td>much</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>fun</td>\n      <td>fun</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>!</td>\n      <td>!</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# spacy.cli.download(\"en_core_web_md\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\") # load english model\n",
    "\n",
    "document = nlp(\"EEBDA is SO much fun!\")\n",
    "\n",
    "pd.DataFrame({\"Token\": [word.text for word in document],\n",
    "              \"Base\": [word.lemma_ for word in document]})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T22:30:25.907704Z",
     "start_time": "2023-12-16T22:30:24.356737Z"
    }
   },
   "id": "fad1456e99a34931"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14e453b57dbbb5b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Words are now mapped n-dimensionally (vectors), which allows the decomposition of single words."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa945e2acc23d03d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([  1.233  ,   4.2963 ,  -7.9738 , -10.121  ,   1.8207 ,   1.4098 ,\n        -4.518  ,  -5.2261 ,  -0.29157,   0.95234], dtype=float32)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nlp(\"dog\")\n",
    "    \n",
    "embed.vector[0:10] # show first 10 entries for embedding"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T22:33:44.456475Z",
     "start_time": "2023-12-16T22:33:44.415541Z"
    }
   },
   "id": "294e815246c37267"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8220816752553904"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = nlp(\"dog\")\n",
    "doc2 = nlp(\"cat\")\n",
    "\n",
    "# Similarity of two words\n",
    "doc1.similarity(doc2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T22:34:23.402446Z",
     "start_time": "2023-12-16T22:34:23.346628Z"
    }
   },
   "id": "c55fb42cae16f654"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
